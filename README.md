# TestChatBot - Chatbot EstadÃ­stico ProbabilÃ­stico desde Cero

> "Dejo que Copilot haga mi trabajo creando un chatbot como prueba : 3"

## ğŸ¯ DescripciÃ³n del Proyecto

Chatbot estadÃ­stico probabilÃ­stico construido **completamente desde cero** para responder preguntas sobre informaciÃ³n universitaria, especÃ­ficamente sobre cÃ³mo navegar y encontrar informaciÃ³n en el sitio web de la universidad.

### CaracterÃ­sticas principales:
- âœ… **Modelo desde cero**: Sin usar LLMs pre-entrenados
- âœ… **EstadÃ­stico y probabilÃ­stico**: Basado en cÃ¡lculos de probabilidad
- âœ… **Scope limitado**: Solo responde preguntas relacionadas con la universidad
- âœ… **Entrenamiento incremental**: DiseÃ±ado para entrenar en sesiones nocturnas
- âœ… **Escalable**: Preparado para continuar entrenamiento en servidor

---

## ğŸ§  Arquitectura del Modelo

### Tipo de Modelo: LSTM Bidireccional Encoder-Decoder con Attention

```
Pregunta del usuario
      â†“
[TokenizaciÃ³n]
      â†“
[Embedding Layer] (entrenado desde cero)
      â†“
[LSTM Encoder Bidireccional]
      â†“
[Context Vector + Attention Mechanism]
      â†“
[LSTM Decoder]
      â†“
[Capa de Salida con Softmax]
      â†“
[Sampling ProbabilÃ­stico]
      â†“
Respuesta generada
```

### Componentes construidos desde cero:
1. **Tokenizador custom** - Vocabulario especÃ­fico del dominio
2. **Embeddings** - Vectores de palabras entrenables
3. **LSTM Encoder** - Procesa la pregunta
4. **Attention Mechanism** - Focaliza en partes relevantes
5. **LSTM Decoder** - Genera la respuesta palabra por palabra
6. **Filtro de Scope** - Rechaza preguntas fuera del dominio

---

## ğŸ“ Estructura del Proyecto

```
TestChatBot/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ qa_dataset.txt          # Dataset con preguntas y respuestas
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ train.pkl               # Datos de entrenamiento procesados
â”‚   â”‚   â”œâ”€â”€ val.pkl                 # Datos de validaciÃ³n
â”‚   â”‚   â””â”€â”€ vocab.json              # Vocabulario generado
â”‚   â””â”€â”€ scope_keywords.json         # Palabras clave del dominio
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/                # Guardado automÃ¡tico durante entrenamiento
â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_1.pt
â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_5.pt
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”œâ”€â”€ tokenizer/
â”‚   â”‚   â””â”€â”€ tokenizer.pkl           # Tokenizador entrenado
â”‚   â””â”€â”€ final/
â”‚       â””â”€â”€ best_model.pt           # Mejor modelo
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py            # Limpieza y preparaciÃ³n de datos
â”‚   â”œâ”€â”€ tokenizer.py                # Tokenizador custom
â”‚   â”œâ”€â”€ embeddings.py               # Capa de embeddings
â”‚   â”œâ”€â”€ encoder.py                  # LSTM Encoder
â”‚   â”œâ”€â”€ decoder.py                  # LSTM Decoder con Attention
â”‚   â”œâ”€â”€ model.py                    # Modelo completo Seq2Seq
â”‚   â”œâ”€â”€ train.py                    # Script de entrenamiento
â”‚   â”œâ”€â”€ inference.py                # GeneraciÃ³n de respuestas
â”‚   â””â”€â”€ scope_filter.py             # Filtro de relevancia
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py                      # API REST con FastAPI
â”‚   â”œâ”€â”€ chatbot.py                  # LÃ³gica del chatbot
â”‚   â””â”€â”€ static/                     # Frontend web simple
â”‚       â”œâ”€â”€ index.html
â”‚       â”œâ”€â”€ style.css
â”‚       â””â”€â”€ script.js
â”‚
â”œâ”€â”€ config.yaml                     # ConfiguraciÃ³n de hiperparÃ¡metros
â”œâ”€â”€ requirements.txt                # Dependencias del proyecto
â”œâ”€â”€ train.sh                        # Script para entrenar fÃ¡cilmente
â”œâ”€â”€ README.md                       # Este archivo
â””â”€â”€ .gitignore
```

---

## ğŸ› ï¸ Stack TecnolÃ³gico

### Lenguaje y Framework
- **Python 3.8+**
- **PyTorch** (para construir el modelo desde cero)

### Bibliotecas principales
- `torch` - Framework de deep learning
- `numpy` - Operaciones numÃ©ricas
- `nltk` - Procesamiento de lenguaje natural
- `scikit-learn` - MÃ©tricas y preprocesamiento
- `pyyaml` - ConfiguraciÃ³n
- `tqdm` - Barras de progreso
- `matplotlib` / `seaborn` - VisualizaciÃ³n

### API y Frontend
- `fastapi` - API REST
- `uvicorn` - Servidor ASGI
- HTML/CSS/JavaScript vanilla

---

## âš™ï¸ ConfiguraciÃ³n del Modelo

### HiperparÃ¡metros (config.yaml)

```yaml
model:
  embedding_dim: 256          # DimensiÃ³n de embeddings
  hidden_dim: 512             # DimensiÃ³n de LSTM
  num_layers: 2               # Capas de LSTM
  dropout: 0.3                # Dropout para regularizaciÃ³n
  bidirectional: true         # LSTM bidireccional en encoder
  attention: true             # Usar mecanismo de attention

training:
  batch_size: 32
  learning_rate: 0.001
  num_epochs: 100             # Entrenamientos nocturnos
  gradient_clip: 5.0
  checkpoint_every: 5         # Guardar cada 5 Ã©pocas
  early_stopping_patience: 10

data:
  train_split: 0.8
  val_split: 0.2
  max_seq_length: 50          # Longitud mÃ¡xima de secuencias
  min_word_freq: 2            # Frecuencia mÃ­nima para vocabulario

scope_filter:
  similarity_threshold: 0.6   # Umbral para aceptar preguntas
  keywords_file: "data/scope_keywords.json"
```

---

## ğŸš€ Uso

### 1. InstalaciÃ³n

```bash
# Clonar repositorio
git clone https://github.com/Nnico0w0/TestChatBot.git
cd TestChatBot

# Crear entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# Instalar dependencias
pip install -r requirements.txt
```

### 2. Preprocesamiento

```bash
python src/preprocessing.py
```

Esto generarÃ¡:
- `data/processed/train.pkl` - Datos de entrenamiento
- `data/processed/val.pkl` - Datos de validaciÃ³n

### 3. Construir Tokenizador

```bash
python src/tokenizer.py
```

Esto generarÃ¡:
- `models/tokenizer/tokenizer.pkl` - Tokenizador entrenado
- `data/processed/vocab.json` - Vocabulario del modelo

### 4. Entrenamiento

```bash
# Entrenamiento simple
python src/train.py

# O usar el script preparado
bash train.sh
```

**Para entrenamientos nocturnos:**
```bash
# En Linux/Mac
nohup python src/train.py > training.log 2>&1 &

# En Windows (PowerShell)
Start-Process python -ArgumentList "src/train.py" -RedirectStandardOutput "training.log" -NoNewWindow
```

### 5. Inferencia (probar el chatbot)

```bash
# Modo interactivo
python src/inference.py

# Pregunta Ãºnica
python src/inference.py --question "Â¿CuÃ¡ndo comienzan las inscripciones?"
```

### 6. Ejecutar API Web

```bash
uvicorn app.api:app --reload
# Abrir http://localhost:8000 en el navegador
```

---

## ğŸ“Š Dataset

### Formato del archivo `qa_dataset.txt`

El dataset estÃ¡ estructurado en formato JSON con la siguiente estructura:

```json
[
  {
    "intent": "fechas_inscripcion",
    "questions": [
      "Â¿CuÃ¡ndo comienzan las inscripciones a la universidad?",
      "Â¿CuÃ¡ndo abren las inscripciones?",
      "Â¿CuÃ¡ndo puedo anotarme en la universidad?"
    ],
    "answer": "Las inscripciones comienzan en las fechas publicadas cada aÃ±o por la instituciÃ³n..."
  }
]
```

### Intents incluidos:
- InscripciÃ³n y admisiÃ³n
- Fechas importantes
- Requisitos y documentaciÃ³n
- NavegaciÃ³n del sitio web
- InformaciÃ³n de carreras
- Consultas administrativas

---

## ğŸ¯ Filtro de Scope

El chatbot incluye un **filtro de relevancia** que:

1. âœ… Calcula la similitud semÃ¡ntica entre la pregunta y el dominio
2. âœ… Compara con palabras clave del scope universitario
3. âœ… Rechaza cortÃ©smente preguntas fuera del tema

**Ejemplo:**
```
Usuario: "Â¿CuÃ¡l es la capital de Francia?"
Bot: "Lo siento, solo puedo responder preguntas sobre informaciÃ³n 
     universitaria y navegaciÃ³n del sitio web de la instituciÃ³n."
```

---

## ğŸ“ˆ Sistema de Checkpoints

Durante el entrenamiento nocturno, el modelo guarda automÃ¡ticamente:

- âœ… **Checkpoint cada N Ã©pocas** (configurable)
- âœ… **Mejor modelo** segÃºn pÃ©rdida de validaciÃ³n
- âœ… **MÃ©tricas de entrenamiento** (loss, accuracy, perplexity)
- âœ… **Estado del optimizador** (para continuar entrenamiento)

Si el entrenamiento se interrumpe, puedes continuar desde el Ãºltimo checkpoint:

```bash
python src/train.py --resume --checkpoint models/checkpoints/checkpoint_epoch_20.pt
```

---

## ğŸ§ª EvaluaciÃ³n del Modelo

### MÃ©tricas utilizadas:
- **Perplexity** - Mide quÃ© tan "sorprendido" estÃ¡ el modelo
- **Loss de validaciÃ³n** - Error en datos no vistos

---

## ğŸ”¬ Fundamentos TeÃ³ricos

### Â¿Por quÃ© es "estadÃ­stico probabilÃ­stico"?

1. **Embeddings probabilÃ­sticos**: Cada palabra se representa como un vector en un espacio de probabilidades

2. **LSTM calcula probabilidades**: En cada paso temporal, calcula la probabilidad de la siguiente palabra dado el contexto

3. **Softmax**: Convierte las salidas en distribuciÃ³n de probabilidad sobre todo el vocabulario

4. **Sampling**: La respuesta se genera muestreando de la distribuciÃ³n de probabilidad (no es determinÃ­stico)

### Ecuaciones clave:

**LSTM Cell:**
```
f_t = Ïƒ(W_f Â· [h_{t-1}, x_t] + b_f)  # Forget gate
i_t = Ïƒ(W_i Â· [h_{t-1}, x_t] + b_i)  # Input gate
o_t = Ïƒ(W_o Â· [h_{t-1}, x_t] + b_o)  # Output gate
c_t = f_t * c_{t-1} + i_t * tanh(W_c Â· [h_{t-1}, x_t] + b_c)
h_t = o_t * tanh(c_t)
```

**Attention:**
```
score(h_t, h_s) = h_t^T Â· W_a Â· h_s
Î±_t = softmax(score(h_t, h_s))
context_t = Î£(Î±_t * h_s)
```

---

## ğŸ–¥ï¸ Requisitos de Hardware

### MÃ­nimo (CPU):
- 8GB RAM
- Entrenamiento lento (dÃ­as)

### Recomendado (GPU):
- ASUS TUF A15 o similar
- GPU NVIDIA (RTX 3050+)
- 16GB RAM
- Entrenamiento: 2-7 noches

### Ã“ptimo (Servidor):
- GPU NVIDIA (RTX 3080+)
- 32GB+ RAM
- Entrenamiento: horas

---

## ğŸ¤ Contribuciones

Este es un proyecto educativo. Sugerencias y mejoras son bienvenidas.

---

## ğŸ“„ Licencia

MIT License

---

## ğŸ‘¤ Autor

**Nnico0w0**

GitHub: [@Nnico0w0](https://github.com/Nnico0w0)

---

## ğŸ“š Referencias

### Papers y recursos:
- [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)
- [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
- [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

---

## ğŸ“ Notas del Proyecto

Este chatbot es un proyecto educativo que demuestra:
- ConstrucciÃ³n de modelos de NLP desde cero
- Arquitecturas Encoder-Decoder
- Mecanismos de Attention
- Entrenamiento de modelos secuenciales
- Filtrado de dominio
- Despliegue de modelos

**Objetivo**: Aprender los fundamentos de los chatbots estadÃ­sticos probabilÃ­sticos sin depender de modelos pre-entrenados, construyendo cada componente desde cero para entender profundamente cÃ³mo funcionan.

---

**Â¡Empecemos a entrenar!** ğŸš€
