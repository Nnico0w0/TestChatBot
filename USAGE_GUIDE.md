# Gu√≠a de Uso - TestChatBot

Esta gu√≠a explica c√≥mo usar el chatbot desde la instalaci√≥n hasta el despliegue.

## üìã Requisitos Previos

- Python 3.8 o superior
- pip (gestor de paquetes de Python)
- 8GB RAM m√≠nimo (16GB recomendado)
- GPU NVIDIA (opcional, pero recomendado para entrenamiento)

## üöÄ Instalaci√≥n R√°pida

### 1. Clonar el Repositorio

```bash
git clone https://github.com/Nnico0w0/TestChatBot.git
cd TestChatBot
```

### 2. Crear Entorno Virtual

```bash
# En Linux/Mac
python -m venv venv
source venv/bin/activate

# En Windows
python -m venv venv
venv\Scripts\activate
```

### 3. Instalar Dependencias

```bash
pip install -r requirements.txt
```

## üìä Preparar los Datos

### Paso 1: Preprocesamiento

```bash
python src/preprocessing.py
```

**Salida esperada:**
```
Iniciando preprocesamiento...
Leyendo dataset desde: data/raw/qa_dataset.txt
Intents encontrados: 16
Pares de Q&A creados: 60
Datos de entrenamiento: 48
Datos de validaci√≥n: 12
Datos guardados en: data/processed
Preprocesamiento completado exitosamente!
```

### Paso 2: Construir Tokenizador

```bash
python src/tokenizer.py
```

**Salida esperada:**
```
Construyendo vocabulario...
Total de palabras √∫nicas: 253
Tama√±o del vocabulario: 199
Tokenizador guardado en: models/tokenizer/tokenizer.pkl
Vocabulario guardado en: data/processed/vocab.json
Tokenizador construido exitosamente!
```

## üéØ Entrenar el Modelo

### Entrenamiento B√°sico

```bash
# Usando el script
bash train.sh

# O directamente con Python
PYTHONPATH=. python src/train.py
```

### Entrenamiento Personalizado

```bash
# Entrenar por N √©pocas espec√≠ficas
PYTHONPATH=. python src/train.py --epochs 50

# Con configuraci√≥n personalizada
PYTHONPATH=. python src/train.py --config config_custom.yaml
```

### Entrenamiento Nocturno (Linux/Mac)

```bash
# Ejecutar en background con logging
nohup python src/train.py > training.log 2>&1 &

# Ver el progreso
tail -f training.log
```

### Entrenamiento Nocturno (Windows)

```powershell
# En PowerShell
Start-Process python -ArgumentList "src/train.py" -RedirectStandardOutput "training.log" -NoNewWindow

# Ver el log
Get-Content training.log -Wait
```

### Salida Durante el Entrenamiento

```
Usando dispositivo: cpu
Cargando tokenizer...
Cargando datos...
Creando modelo...

Iniciando entrenamiento por 100 √©pocas...
Tama√±o del vocabulario: 199
Datos de entrenamiento: 48
Datos de validaci√≥n: 12

√âpoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:05<00:00,  2.79s/it]
√âpoca 1/100
Train Loss: 5.2627 | Train Perplexity: 192.9982
Val Loss: 4.9158 | Val Perplexity: 136.4233
Mejor modelo guardado: models/final/best_model.pt

...
```

## üí¨ Usar el Chatbot

### Modo Interactivo (Consola)

```bash
PYTHONPATH=. python src/inference.py
```

**Ejemplo de uso:**
```
==================================================
Chatbot Universitario - Modo Interactivo
==================================================
Escribe 'salir' o 'exit' para terminar

T√∫: ¬øCu√°ndo comienzan las inscripciones?
Bot: Las inscripciones comienzan en las fechas publicadas cada a√±o por la instituci√≥n...

T√∫: ¬øCu√°l es la capital de Francia?
Bot: Lo siento, solo puedo responder preguntas sobre informaci√≥n universitaria...

T√∫: salir
¬°Hasta luego!
```

### Pregunta √önica

```bash
PYTHONPATH=. python src/inference.py --question "¬øD√≥nde puedo ver el calendario acad√©mico?"
```

## üåê API Web

### Iniciar el Servidor

```bash
# Con uvicorn directamente
uvicorn app.api:app --reload --host 0.0.0.0 --port 8000

# O usando Python
PYTHONPATH=. python app/api.py
```

### Acceder a la Interfaz Web

Abrir en el navegador: `http://localhost:8000`

### Documentaci√≥n de la API

Swagger UI: `http://localhost:8000/docs`

### Usar la API

#### Endpoint POST /chat

```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "question": "¬øCu√°ndo comienzan las inscripciones?",
    "max_length": 50
  }'
```

**Respuesta:**
```json
{
  "question": "¬øCu√°ndo comienzan las inscripciones?",
  "answer": "Las inscripciones comienzan en las fechas publicadas...",
  "in_scope": true,
  "confidence": 1.0
}
```

#### Endpoint GET /health

```bash
curl http://localhost:8000/health
```

**Respuesta:**
```json
{
  "status": "healthy",
  "model_loaded": true
}
```

## üîß Configuraci√≥n Avanzada

### Modificar Hiperpar√°metros

Editar `config.yaml`:

```yaml
model:
  embedding_dim: 256      # Aumentar para mejor representaci√≥n
  hidden_dim: 512         # Aumentar para mayor capacidad
  num_layers: 2           # M√°s capas = m√°s complejidad
  dropout: 0.3            # Reducir si hay underfitting

training:
  batch_size: 32          # Ajustar seg√∫n RAM disponible
  learning_rate: 0.001    # Reducir si el loss oscila mucho
  num_epochs: 100         # Aumentar para mejor convergencia
```

### Agregar Nuevas Keywords

Editar `data/scope_keywords.json`:

```json
{
  "keywords": [
    "universidad",
    "inscripci√≥n",
    "tu_nueva_keyword"
  ],
  "rejection_messages": [
    "Tu mensaje personalizado de rechazo"
  ]
}
```

### Expandir el Dataset

Editar `data/raw/qa_dataset.txt` y agregar nuevos intents:

```json
[
  {
    "intent": "nuevo_intent",
    "questions": [
      "¬øPregunta ejemplo 1?",
      "¬øPregunta ejemplo 2?"
    ],
    "answer": "Respuesta para este intent."
  }
]
```

Luego, re-ejecutar preprocesamiento y tokenizador:

```bash
python src/preprocessing.py
python src/tokenizer.py
```

## üìà Monitoreo del Entrenamiento

### Checkpoints

Los checkpoints se guardan autom√°ticamente en `models/checkpoints/`:
- `checkpoint_epoch_5.pt`
- `checkpoint_epoch_10.pt`
- etc.

### Mejor Modelo

El mejor modelo (menor p√©rdida de validaci√≥n) se guarda en:
- `models/final/best_model.pt`

### Continuar Entrenamiento (TODO)

Para continuar desde un checkpoint:

```bash
PYTHONPATH=. python src/train.py --resume --checkpoint models/checkpoints/checkpoint_epoch_20.pt
```

## üêõ Soluci√≥n de Problemas

### Error: "ModuleNotFoundError: No module named 'src'"

**Soluci√≥n:** Usar `PYTHONPATH=.` antes del comando:

```bash
PYTHONPATH=. python src/train.py
```

### Error: "Resource punkt_tab not found"

**Soluci√≥n:** El tokenizador autom√°ticamente descarga los recursos necesarios. Si persiste:

```bash
python -c "import nltk; nltk.download('punkt_tab')"
```

### Error: "Model not found"

**Soluci√≥n:** Asegurarse de entrenar el modelo primero:

```bash
PYTHONPATH=. python src/train.py --epochs 10
```

### Respuestas Incoherentes

**Causa:** El modelo necesita m√°s entrenamiento.

**Soluci√≥n:** 
1. Aumentar el n√∫mero de √©pocas
2. Expandir el dataset
3. Ajustar hiperpar√°metros

### Out of Memory (OOM)

**Soluci√≥n:**
1. Reducir `batch_size` en `config.yaml`
2. Reducir `hidden_dim` o `embedding_dim`
3. Usar GPU con m√°s memoria

## üì¶ Despliegue

### Docker (TODO)

```bash
# Construir imagen
docker build -t testchatbot .

# Ejecutar contenedor
docker run -p 8000:8000 testchatbot
```

### Servidor de Producci√≥n

```bash
# Instalar gunicorn
pip install gunicorn

# Ejecutar con gunicorn
gunicorn app.api:app -w 4 -k uvicorn.workers.UvicornWorker -b 0.0.0.0:8000
```

## üìù Notas Importantes

1. **Primera Ejecuci√≥n**: El primer entrenamiento puede ser lento mientras PyTorch configura el backend.

2. **GPU vs CPU**: El entrenamiento en CPU es funcional pero lento. GPU es altamente recomendada para entrenamientos largos.

3. **Dataset Peque√±o**: El dataset actual es peque√±o (60 pares Q&A). Para mejores resultados, expandir a 500+ pares.

4. **Modelo Pre-entrenado**: Este proyecto NO usa modelos pre-entrenados. Todo se entrena desde cero.

5. **Scope Filter**: El filtro es simple pero efectivo. Para mayor precisi√≥n, considerar usar embeddings sem√°nticos.

## üéì Aprendizaje

Este proyecto es educativo. Los conceptos principales implementados:

- ‚úÖ Tokenizaci√≥n y vocabulario
- ‚úÖ Embeddings de palabras
- ‚úÖ Arquitectura Encoder-Decoder
- ‚úÖ Mecanismo de Attention
- ‚úÖ LSTM bidireccional
- ‚úÖ Training loop con validaci√≥n
- ‚úÖ Checkpointing
- ‚úÖ Early stopping
- ‚úÖ API REST
- ‚úÖ Filtrado de dominio

## üìû Soporte

Para preguntas o problemas:
- Abrir un issue en GitHub
- Revisar la documentaci√≥n en README.md
- Verificar los logs de entrenamiento

---

**¬°Buena suerte con tu chatbot!** üöÄ
