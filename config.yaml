model:
  embedding_dim: 256          # Dimensión de embeddings
  hidden_dim: 512             # Dimensión de LSTM
  num_layers: 2               # Capas de LSTM
  dropout: 0.3                # Dropout para regularización
  bidirectional: true         # LSTM bidireccional en encoder
  attention: true             # Usar mecanismo de attention

training:
  batch_size: 32
  learning_rate: 0.001
  num_epochs: 100             # Entrenamientos nocturnos
  gradient_clip: 5.0
  checkpoint_every: 5         # Guardar cada 5 épocas
  early_stopping_patience: 10

data:
  train_split: 0.8
  val_split: 0.2
  max_seq_length: 50          # Longitud máxima de secuencias
  min_word_freq: 2            # Frecuencia mínima para vocabulario
  raw_data_path: "data/raw/qa_dataset.txt"
  processed_data_path: "data/processed/"
  vocab_path: "data/processed/vocab.json"

scope_filter:
  similarity_threshold: 0.6   # Umbral para aceptar preguntas
  keywords_file: "data/scope_keywords.json"

paths:
  checkpoints_dir: "models/checkpoints/"
  tokenizer_dir: "models/tokenizer/"
  final_model_dir: "models/final/"
  best_model_path: "models/final/best_model.pt"
